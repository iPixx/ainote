//! Advanced Memory Management System for aiNote
//!
//! This module provides comprehensive memory management capabilities including:
//! - Memory leak detection and prevention
//! - Automatic garbage collection optimization 
//! - Memory allocation limits for AI operations
//! - Real-time memory monitoring and alerting
//! - Smart cache eviction policies
//!
//! ## Performance Targets
//! - Base memory usage <100MB (excluding Ollama)
//! - Memory cleanup within 5s of operation completion  
//! - No memory leaks detected in stress testing
//! - Cache hit rate >80% for embedding retrieval

use std::collections::HashMap;
use std::sync::{Arc, atomic::{AtomicUsize, AtomicBool, Ordering}};
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};
use thiserror::Error;

use crate::performance::PerformanceTracker;

/// Memory management errors
#[derive(Error, Debug, Clone)]
pub enum MemoryError {
    #[error("Memory limit exceeded: {used_mb}MB > {limit_mb}MB")]
    MemoryLimitExceeded { used_mb: usize, limit_mb: usize },
    
    #[error("Memory leak detected: {leak_size_mb}MB in {component}")]
    MemoryLeakDetected { component: String, leak_size_mb: f64 },
    
    #[error("Allocation failed: {requested_mb}MB not available")]
    AllocationFailed { requested_mb: usize },
    
    #[error("GC operation failed: {message}")]
    GcFailed { message: String },
    
    #[error("Memory monitor not initialized")]
    MonitorNotInitialized,
}

pub type MemoryResult<T> = Result<T, MemoryError>;

/// Memory management configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryManagerConfig {
    /// Maximum total memory usage in MB
    pub max_memory_mb: usize,
    /// Memory limit for AI operations in MB
    pub ai_operations_limit_mb: usize,
    /// Memory monitoring interval in seconds
    pub monitoring_interval_seconds: u64,
    /// Enable automatic garbage collection
    pub enable_auto_gc: bool,
    /// GC trigger threshold (percentage of max memory)
    pub gc_trigger_threshold_percent: f64,
    /// Enable memory leak detection
    pub enable_leak_detection: bool,
    /// Leak detection threshold in MB
    pub leak_detection_threshold_mb: f64,
    /// Memory alert threshold percentage
    pub alert_threshold_percent: f64,
    /// Cache cleanup interval in seconds
    pub cache_cleanup_interval_seconds: u64,
}

impl Default for MemoryManagerConfig {
    fn default() -> Self {
        Self {
            max_memory_mb: 100,                    // 100MB base target
            ai_operations_limit_mb: 50,            // 50MB for AI operations
            monitoring_interval_seconds: 10,       // Monitor every 10 seconds
            enable_auto_gc: true,                  // Auto GC enabled
            gc_trigger_threshold_percent: 75.0,    // Trigger GC at 75%
            enable_leak_detection: true,           // Leak detection enabled
            leak_detection_threshold_mb: 10.0,     // Alert on 10MB leaks
            alert_threshold_percent: 85.0,         // Alert at 85%
            cache_cleanup_interval_seconds: 300,   // Clean cache every 5 minutes
        }
    }
}

/// Memory allocation tracking
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryAllocation {
    /// Allocation ID
    pub id: String,
    /// Component that allocated memory
    pub component: String,
    /// Size in bytes
    pub size_bytes: usize,
    /// Allocation timestamp
    pub allocated_at: u64,
    /// Last access timestamp
    pub last_accessed: u64,
    /// Allocation type
    pub allocation_type: AllocationType,
    /// Whether allocation is still active
    pub is_active: bool,
}

/// Types of memory allocations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AllocationType {
    /// Embedding cache allocation
    EmbeddingCache,
    /// Vector storage allocation
    VectorStorage,
    /// AI operation temporary allocation
    AiOperation,
    /// Background process allocation
    BackgroundProcess,
    /// File operation allocation
    FileOperation,
    /// Search result allocation
    SearchResult,
}

/// Memory usage metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryMetrics {
    /// Total memory usage in MB
    pub total_memory_mb: f64,
    /// AI operations memory usage in MB  
    pub ai_operations_memory_mb: f64,
    /// Cache memory usage in MB
    pub cache_memory_mb: f64,
    /// Free memory in MB
    pub free_memory_mb: f64,
    /// Memory usage percentage
    pub usage_percentage: f64,
    /// Number of active allocations
    pub active_allocations: usize,
    /// Number of detected leaks
    pub detected_leaks: usize,
    /// Last GC timestamp
    pub last_gc_timestamp: u64,
    /// Memory pressure level (0.0 to 1.0)
    pub memory_pressure: f64,
    /// Timestamp of metrics
    pub timestamp: u64,
}

/// Memory leak detection entry
#[derive(Debug, Clone)]
struct LeakDetectionEntry {
    _component: String,
    initial_size: usize,
    last_check_time: Instant,
    growth_rate_mb_per_sec: f64,
    consecutive_growth_checks: usize,
}

/// Memory allocation limiter for AI operations
#[derive(Debug)]
pub struct AllocationLimiter {
    current_ai_usage: Arc<AtomicUsize>,
    max_ai_limit_bytes: usize,
    active_ai_allocations: Arc<RwLock<HashMap<String, usize>>>,
}

impl AllocationLimiter {
    pub fn new(max_ai_limit_mb: usize) -> Self {
        Self {
            current_ai_usage: Arc::new(AtomicUsize::new(0)),
            max_ai_limit_bytes: max_ai_limit_mb * 1024 * 1024,
            active_ai_allocations: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// Request AI memory allocation
    pub async fn request_ai_allocation(&self, operation_id: &str, size_bytes: usize) -> MemoryResult<()> {
        let current = self.current_ai_usage.load(Ordering::Acquire);
        
        if current + size_bytes > self.max_ai_limit_bytes {
            return Err(MemoryError::AllocationFailed {
                requested_mb: size_bytes / (1024 * 1024),
            });
        }

        // Reserve the memory
        self.current_ai_usage.fetch_add(size_bytes, Ordering::AcqRel);
        
        // Track the allocation
        let mut allocations = self.active_ai_allocations.write().await;
        allocations.insert(operation_id.to_string(), size_bytes);
        
        Ok(())
    }

    /// Release AI memory allocation
    pub async fn release_ai_allocation(&self, operation_id: &str) -> MemoryResult<()> {
        let mut allocations = self.active_ai_allocations.write().await;
        
        if let Some(size_bytes) = allocations.remove(operation_id) {
            self.current_ai_usage.fetch_sub(size_bytes, Ordering::AcqRel);
        }
        
        Ok(())
    }

    /// Get current AI memory usage in MB
    pub fn get_ai_usage_mb(&self) -> f64 {
        self.current_ai_usage.load(Ordering::Acquire) as f64 / (1024.0 * 1024.0)
    }
}

/// Main memory management system
pub struct MemoryManager {
    config: MemoryManagerConfig,
    allocation_tracker: Arc<RwLock<HashMap<String, MemoryAllocation>>>,
    allocation_limiter: AllocationLimiter,
    leak_detection: Arc<RwLock<HashMap<String, LeakDetectionEntry>>>,
    metrics_history: Arc<RwLock<Vec<MemoryMetrics>>>,
    is_running: Arc<AtomicBool>,
    monitoring_handle: Option<tokio::task::JoinHandle<()>>,
    cleanup_handle: Option<tokio::task::JoinHandle<()>>,
}

impl MemoryManager {
    /// Create new memory manager
    pub fn new(config: MemoryManagerConfig) -> Self {
        let allocation_limiter = AllocationLimiter::new(config.ai_operations_limit_mb);
        
        Self {
            config,
            allocation_tracker: Arc::new(RwLock::new(HashMap::new())),
            allocation_limiter,
            leak_detection: Arc::new(RwLock::new(HashMap::new())),
            metrics_history: Arc::new(RwLock::new(Vec::new())),
            is_running: Arc::new(AtomicBool::new(false)),
            monitoring_handle: None,
            cleanup_handle: None,
        }
    }

    /// Start memory monitoring
    pub async fn start(&mut self) -> MemoryResult<()> {
        if self.is_running.load(Ordering::Acquire) {
            return Ok(());
        }

        self.is_running.store(true, Ordering::Release);
        
        // Start memory monitoring task
        self.start_monitoring_task().await;
        
        // Start cleanup task
        self.start_cleanup_task().await;
        
        eprintln!("ðŸ§  Memory manager started with {}MB limit", self.config.max_memory_mb);
        Ok(())
    }

    /// Stop memory monitoring
    pub async fn stop(&mut self) -> MemoryResult<()> {
        self.is_running.store(false, Ordering::Release);
        
        if let Some(handle) = self.monitoring_handle.take() {
            handle.abort();
        }
        
        if let Some(handle) = self.cleanup_handle.take() {
            handle.abort();
        }
        
        eprintln!("ðŸ§  Memory manager stopped");
        Ok(())
    }

    /// Track memory allocation
    pub async fn track_allocation(
        &self,
        allocation_id: String,
        component: String,
        size_bytes: usize,
        allocation_type: AllocationType,
    ) -> MemoryResult<()> {
        let tracker = PerformanceTracker::start("track_allocation");
        
        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();

        let allocation = MemoryAllocation {
            id: allocation_id.clone(),
            component: component.clone(),
            size_bytes,
            allocated_at: timestamp,
            last_accessed: timestamp,
            allocation_type,
            is_active: true,
        };

        {
            let mut allocations = self.allocation_tracker.write().await;
            allocations.insert(allocation_id, allocation);
        } // Release write lock before checking limits
        
        tracker.finish();
        
        // TODO: Re-enable memory limit checking after fixing potential deadlock
        // Issue: check_memory_limits -> get_memory_metrics -> allocation_tracker.read()
        // but we may still hold locks from track_allocation chain
        // self.check_memory_limits().await?;
        
        Ok(())
    }

    /// Release memory allocation
    pub async fn release_allocation(&self, allocation_id: &str) -> MemoryResult<()> {
        let mut allocations = self.allocation_tracker.write().await;
        
        if let Some(mut allocation) = allocations.remove(allocation_id) {
            allocation.is_active = false;
            
            // If it's an AI allocation, release from limiter
            if matches!(allocation.allocation_type, AllocationType::AiOperation) {
                self.allocation_limiter.release_ai_allocation(allocation_id).await?;
            }
        }
        
        Ok(())
    }

    /// Request AI operation memory allocation
    pub async fn request_ai_allocation(&self, operation_id: &str, size_bytes: usize) -> MemoryResult<()> {
        // Check allocation limiter first
        self.allocation_limiter.request_ai_allocation(operation_id, size_bytes).await?;
        
        // Track the allocation
        self.track_allocation(
            operation_id.to_string(),
            "ai_operation".to_string(),
            size_bytes,
            AllocationType::AiOperation,
        ).await?;
        
        Ok(())
    }

    /// Get current memory metrics
    pub async fn get_memory_metrics(&self) -> MemoryResult<MemoryMetrics> {
        let tracker = PerformanceTracker::start("get_memory_metrics");
        
        let allocations = self.allocation_tracker.read().await;
        
        let total_bytes: usize = allocations.values()
            .filter(|a| a.is_active)
            .map(|a| a.size_bytes)
            .sum();

        let ai_bytes: usize = allocations.values()
            .filter(|a| a.is_active && matches!(a.allocation_type, AllocationType::AiOperation))
            .map(|a| a.size_bytes)
            .sum();

        let cache_bytes: usize = allocations.values()
            .filter(|a| a.is_active && matches!(a.allocation_type, AllocationType::EmbeddingCache))
            .map(|a| a.size_bytes)
            .sum();

        let total_mb = total_bytes as f64 / (1024.0 * 1024.0);
        let ai_mb = ai_bytes as f64 / (1024.0 * 1024.0);
        let cache_mb = cache_bytes as f64 / (1024.0 * 1024.0);
        let free_mb = self.config.max_memory_mb as f64 - total_mb;
        let usage_percent = (total_mb / self.config.max_memory_mb as f64) * 100.0;

        let leak_count = self.leak_detection.read().await.len();
        
        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();

        let metrics = MemoryMetrics {
            total_memory_mb: total_mb,
            ai_operations_memory_mb: ai_mb,
            cache_memory_mb: cache_mb,
            free_memory_mb: free_mb.max(0.0),
            usage_percentage: usage_percent,
            active_allocations: allocations.values().filter(|a| a.is_active).count(),
            detected_leaks: leak_count,
            last_gc_timestamp: 0, // TODO: track actual GC timestamp
            memory_pressure: (usage_percent / 100.0).min(1.0),
            timestamp,
        };

        tracker.finish();
        Ok(metrics)
    }

    /// Trigger garbage collection
    pub async fn trigger_gc(&self) -> MemoryResult<usize> {
        let tracker = PerformanceTracker::start("trigger_gc");
        
        let mut cleaned_bytes = 0usize;
        
        // Clean inactive allocations
        {
            let mut allocations = self.allocation_tracker.write().await;
            let current_time = SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs();
            
            let stale_threshold = 300; // 5 minutes
            let mut to_remove = Vec::new();
            
            for (id, allocation) in allocations.iter() {
                if !allocation.is_active || 
                   (current_time - allocation.last_accessed) > stale_threshold {
                    cleaned_bytes += allocation.size_bytes;
                    to_remove.push(id.clone());
                }
            }
            
            for id in to_remove {
                allocations.remove(&id);
            }
        }
        
        // Trigger cache cleanup (would integrate with existing cache systems)
        // This would call cleanup methods on embedding_cache, enhanced_cache, etc.
        
        let cleaned_mb = cleaned_bytes as f64 / (1024.0 * 1024.0);
        
        tracker.finish();
        
        if cleaned_mb > 0.1 {
            eprintln!("ðŸ§¹ GC cleaned up {:.2}MB of memory", cleaned_mb);
        }
        
        Ok(cleaned_bytes)
    }

    /// Detect memory leaks
    pub async fn detect_memory_leaks(&self) -> MemoryResult<Vec<String>> {
        if !self.config.enable_leak_detection {
            return Ok(Vec::new());
        }

        let tracker = PerformanceTracker::start("detect_memory_leaks");
        
        let mut detected_leaks = Vec::new();
        let allocations = self.allocation_tracker.read().await;
        let mut leak_detection = self.leak_detection.write().await;
        
        // Group allocations by component
        let mut component_sizes: HashMap<String, usize> = HashMap::new();
        for allocation in allocations.values().filter(|a| a.is_active) {
            *component_sizes.entry(allocation.component.clone()).or_insert(0) += allocation.size_bytes;
        }
        
        let now = Instant::now();
        
        for (component, current_size) in component_sizes {
            if let Some(entry) = leak_detection.get_mut(&component) {
                // Check for continuous growth
                let time_diff = now.duration_since(entry.last_check_time).as_secs_f64();
                let size_diff = current_size as i64 - entry.initial_size as i64;
                
                if size_diff > 0 && time_diff > 0.0 {
                    let growth_rate = (size_diff as f64) / (1024.0 * 1024.0) / time_diff; // MB/sec
                    entry.growth_rate_mb_per_sec = growth_rate;
                    entry.consecutive_growth_checks += 1;
                    
                    // Detect leak if growing consistently and above threshold
                    if entry.consecutive_growth_checks >= 3 && 
                       growth_rate > 0.01 && // Growing > 0.01 MB/sec
                       (current_size as f64 / (1024.0 * 1024.0)) > self.config.leak_detection_threshold_mb {
                        detected_leaks.push(format!(
                            "Memory leak in {}: {:.2}MB growing at {:.3}MB/s",
                            component, 
                            current_size as f64 / (1024.0 * 1024.0),
                            growth_rate
                        ));
                    }
                } else {
                    entry.consecutive_growth_checks = 0;
                }
                
                entry.initial_size = current_size;
                entry.last_check_time = now;
            } else {
                // New component, start tracking
                leak_detection.insert(component.clone(), LeakDetectionEntry {
                    _component: component,
                    initial_size: current_size,
                    last_check_time: now,
                    growth_rate_mb_per_sec: 0.0,
                    consecutive_growth_checks: 0,
                });
            }
        }
        
        tracker.finish();
        Ok(detected_leaks)
    }

    // Private methods

    #[allow(dead_code)]
    async fn check_memory_limits(&self) -> MemoryResult<()> {
        let metrics = self.get_memory_metrics().await?;
        
        if metrics.usage_percentage > self.config.alert_threshold_percent {
            eprintln!("âš ï¸ Memory usage at {:.1}%", metrics.usage_percentage);
            
            // Trigger GC if auto-enabled and above threshold
            if self.config.enable_auto_gc && 
               metrics.usage_percentage > self.config.gc_trigger_threshold_percent {
                self.trigger_gc().await?;
            }
        }
        
        if metrics.total_memory_mb > self.config.max_memory_mb as f64 {
            return Err(MemoryError::MemoryLimitExceeded {
                used_mb: metrics.total_memory_mb as usize,
                limit_mb: self.config.max_memory_mb,
            });
        }
        
        Ok(())
    }

    async fn start_monitoring_task(&mut self) {
        let allocation_tracker = Arc::clone(&self.allocation_tracker);
        let leak_detection = Arc::clone(&self.leak_detection);
        let metrics_history = Arc::clone(&self.metrics_history);
        let is_running = Arc::clone(&self.is_running);
        let config = self.config.clone();
        
        let handle = tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(config.monitoring_interval_seconds));
            
            while is_running.load(Ordering::Acquire) {
                interval.tick().await;
                
                // Create a temporary MemoryManager instance for monitoring
                let temp_manager = MemoryManager {
                    config: config.clone(),
                    allocation_tracker: Arc::clone(&allocation_tracker),
                    allocation_limiter: AllocationLimiter::new(config.ai_operations_limit_mb),
                    leak_detection: Arc::clone(&leak_detection),
                    metrics_history: Arc::clone(&metrics_history),
                    is_running: Arc::clone(&is_running),
                    monitoring_handle: None,
                    cleanup_handle: None,
                };
                
                // Collect metrics
                if let Ok(metrics) = temp_manager.get_memory_metrics().await {
                    let mut history = metrics_history.write().await;
                    history.push(metrics.clone());
                    
                    // Keep only last 1000 metrics
                    if history.len() > 1000 {
                        history.remove(0);
                    }
                    
                    // Check for memory pressure
                    if metrics.memory_pressure > 0.8 {
                        eprintln!("ðŸš¨ High memory pressure: {:.1}%", metrics.usage_percentage);
                    }
                }
                
                // Check for leaks
                if let Ok(_leaks) = temp_manager.detect_memory_leaks().await {
                    for leak in &_leaks {
                        eprintln!("ðŸš¨ {}", leak);
                    }
                }
            }
        });
        
        self.monitoring_handle = Some(handle);
    }

    async fn start_cleanup_task(&mut self) {
        let is_running = Arc::clone(&self.is_running);
        let config = self.config.clone();
        let allocation_tracker = Arc::clone(&self.allocation_tracker);
        
        let handle = tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(config.cache_cleanup_interval_seconds));
            
            while is_running.load(Ordering::Acquire) {
                interval.tick().await;
                
                // Clean up stale allocations
                let mut allocations = allocation_tracker.write().await;
                let current_time = SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs();
                
                let stale_threshold = 600; // 10 minutes
                let mut cleaned_count = 0;
                
                allocations.retain(|_, allocation| {
                    let is_stale = allocation.is_active && 
                        (current_time - allocation.last_accessed) > stale_threshold;
                    
                    if is_stale {
                        cleaned_count += 1;
                    }
                    
                    !is_stale
                });
                
                if cleaned_count > 0 {
                    eprintln!("ðŸ§¹ Cleaned up {} stale memory allocations", cleaned_count);
                }
            }
        });
        
        self.cleanup_handle = Some(handle);
    }
    
    /// Get metrics history  
    pub async fn get_metrics_history(&self, limit: Option<usize>) -> Vec<MemoryMetrics> {
        let history = self.metrics_history.read().await;
        let limit = limit.unwrap_or(100);
        
        if history.len() <= limit {
            history.clone()
        } else {
            history[history.len() - limit..].to_vec()
        }
    }
}

impl Drop for MemoryManager {
    fn drop(&mut self) {
        self.is_running.store(false, Ordering::Release);
        
        if let Some(handle) = self.monitoring_handle.take() {
            handle.abort();
        }
        
        if let Some(handle) = self.cleanup_handle.take() {
            handle.abort();
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_memory_manager_creation() {
        let config = MemoryManagerConfig::default();
        let manager = MemoryManager::new(config);
        
        let metrics = manager.get_memory_metrics().await.unwrap();
        assert_eq!(metrics.total_memory_mb, 0.0);
        assert_eq!(metrics.active_allocations, 0);
    }
    
    #[tokio::test]
    async fn test_allocation_tracking() {
        let config = MemoryManagerConfig::default();
        let manager = MemoryManager::new(config);
        
        manager.track_allocation(
            "test_allocation".to_string(),
            "test_component".to_string(),
            1024 * 1024, // 1MB
            AllocationType::EmbeddingCache,
        ).await.unwrap();
        
        let metrics = manager.get_memory_metrics().await.unwrap();
        assert_eq!(metrics.active_allocations, 1);
        assert!((metrics.total_memory_mb - 1.0).abs() < 0.1);
    }
    
    #[tokio::test]
    async fn test_ai_allocation_limits() {
        let mut config = MemoryManagerConfig::default();
        config.ai_operations_limit_mb = 10; // 10MB limit
        
        let manager = MemoryManager::new(config);
        
        // Should succeed
        let result = manager.request_ai_allocation("op1", 5 * 1024 * 1024).await;
        assert!(result.is_ok());
        
        // Should fail - exceeds limit
        let result = manager.request_ai_allocation("op2", 8 * 1024 * 1024).await;
        assert!(result.is_err());
    }
    
    #[tokio::test]
    async fn test_garbage_collection() {
        let config = MemoryManagerConfig::default();
        let manager = MemoryManager::new(config);
        
        // Add some allocations
        manager.track_allocation(
            "test1".to_string(),
            "component1".to_string(),
            1024 * 1024,
            AllocationType::EmbeddingCache,
        ).await.unwrap();
        
        manager.track_allocation(
            "test2".to_string(),
            "component2".to_string(),
            1024 * 1024,
            AllocationType::VectorStorage,
        ).await.unwrap();
        
        // Release one allocation
        manager.release_allocation("test1").await.unwrap();
        
        // Trigger GC
        let cleaned = manager.trigger_gc().await.unwrap();
        assert!(cleaned > 0);
    }
    
    #[tokio::test]
    async fn test_memory_leak_detection() {
        let config = MemoryManagerConfig {
            enable_leak_detection: true,
            enable_auto_gc: false, // Disable auto-GC to prevent deadlocks
            max_memory_mb: 1000, // High limit to avoid alerts
            alert_threshold_percent: 99.0, // Very high threshold
            gc_trigger_threshold_percent: 99.0, // Very high threshold
            ..Default::default()
        };
        
        let manager = MemoryManager::new(config);
        
        // Test that detect_memory_leaks runs without hanging on empty tracker
        let leaks = manager.detect_memory_leaks().await.unwrap();
        assert_eq!(leaks.len(), 0);
        
        println!("Memory leak detection test completed successfully");
    }
}